{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0+sJxjDpc+e46I9OS2c0z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edermartelinho/Modelos_Linguagem_Neurais-LLM-/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Dependências"
      ],
      "metadata": {
        "id": "z8dFusTPlHkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "nSTdTpFPlYzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "from tabulate import tabulate\n",
        "from google.colab import widgets as cwidgets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interactive, widgets, interactive_output\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "1bQw5XkglkeH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#* Definições das Funções"
      ],
      "metadata": {
        "id": "Za1-_0Qil7dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COLORS = [5,9,34,62,93,208]\n",
        "\n",
        "# Formata a string para ter uma cor de fundo\n",
        "def colored(txt: str, idx: int):\n",
        "    color = COLORS[idx%len(COLORS)]\n",
        "    return f\"\\x1B[1m\\x1B[48;5;{color}m{txt}\\x1B[0m\""
      ],
      "metadata": {
        "id": "gY7gjK0mmC3u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL_NAME = 'unicamp-dl/translation-en-pt-t5'\n",
        "MODEL_NAME = 'Helsinki-NLP/opus-mt-tc-big-en-pt'\n",
        "# Inicializando o tokenizador e modelo a serem usados\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, output_attentions=True)"
      ],
      "metadata": {
        "id": "FHAVg79wmOFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separa uma string em tokens e imprime cada token com uma cor de fundo\n",
        "def print_colored_tokens(text: str):\n",
        "    # Realiza a tokenização\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    # Imprime os tokens com diferentes cores\n",
        "    print('\\x1B[1mDivisão do Texto\\x1B[0m:\\t', end='')\n",
        "    for idx, token in enumerate(tokens):\n",
        "        print(colored(token.replace(\"▁\", \" \"), idx), end='')\n",
        "\n",
        "    print('\\n\\x1B[1mTokens Ids\\x1B[0m:\\t\\t' + str(tokenizer(text)['input_ids'][:-1]))"
      ],
      "metadata": {
        "id": "xGSIiB2omSxe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=6\n",
        "\n",
        "def print_embeddings(text, hidden_states):\n",
        "    #Tokeniza o texto\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    # Número de camadas de atenção do modelo\n",
        "    n_layers = len(hidden_states)\n",
        "\n",
        "    # Cria uma visualização em abas para cada layer do modelo\n",
        "    tb = cwidgets.TabBar(['inputs'] + ['layer ' + str(i+1) for i in range(n_layers - 1)])\n",
        "    for layer in range(n_layers):\n",
        "        with tb.output_to(layer, select=(layer==0)):\n",
        "\n",
        "            # Inicializa a lista de dados para a tabela de embeddings\n",
        "            # Adiciona os tokens como headers e prepara lista vazias para\n",
        "            # receberem os embeddings\n",
        "            data = []\n",
        "            data.append([colored(token.replace(\"▁\",\"\"), idx) for idx, token in enumerate(tokens)])\n",
        "            [data.append([]) for _ in range(k+1)]\n",
        "\n",
        "            # Itera sobre os tokens e embeddings para preencher a tabela\n",
        "            for i, _ in enumerate(tokens):\n",
        "                # Concatena os primeiros e últimos k//2 valores do embedding\n",
        "                token_embedding = hidden_states[layer][0][i][:k//2].tolist() + hidden_states[layer][0][i][-k//2:].tolist()\n",
        "                for j, val in enumerate(token_embedding):\n",
        "                    data[j+1].append(f\"{val:.3f}\")\n",
        "\n",
        "            # Insere '...' no meio dos valores\n",
        "            data.insert(k//2+1, [\"...\"]*len(tokens))\n",
        "            print(tabulate(data, headers=\"firstrow\", tablefmt=\"plain\"))\n"
      ],
      "metadata": {
        "id": "hPIU2Tdkmbkl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_input_embeddings(text, hidden_states):\n",
        "    #Tokeniza o texto\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    # Inicializa a lista de dados para a tabela de embeddings\n",
        "    # Adiciona os tokens como headers e prepara lista vazias para\n",
        "    # receberem os embeddings\n",
        "\n",
        "    embed_size = 150\n",
        "\n",
        "    fig, axs = plt.subplots(len(tokens), 1, figsize=(15,len(tokens)//1.2))\n",
        "    fig.patch.set_visible(False)\n",
        "    # Itera sobre os tokens e embeddings para preencher a tabela\n",
        "    for i, token in enumerate(tokens):\n",
        "        token_embedding = hidden_states[0][0][i+1, :embed_size].unsqueeze(0).tolist()\n",
        "        img = axs[i].imshow(token_embedding, cmap='Spectral', extent=[0,embed_size,0,embed_size//20])\n",
        "        axs[i].set_frame_on(False)\n",
        "        axs[i].set_xticks([])\n",
        "        axs[i].set_yticks([])\n",
        "        axs[i].set_ylabel(token, rotation=0, verticalalignment='center', horizontalalignment='right')\n",
        "\n",
        "\n",
        "    plt.colorbar(img, ax=axs)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kQDgqZwAm9RY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_encoder_attentions_maps(Input, attentions):\n",
        "    tokens = tokenizer.tokenize(Input)\n",
        "\n",
        "    n_heads = 4\n",
        "\n",
        "    attention = attentions[0][0]\n",
        "    fig, axs = plt.subplots(1, n_heads, figsize=(len(tokens)*2,len(tokens)//1.7), sharey=True)\n",
        "\n",
        "    for head in range(n_heads):\n",
        "        att_matrix = attention[head+4,1:-1,1:-1].detach().numpy()\n",
        "        img = axs[head].imshow(att_matrix, cmap='Blues')\n",
        "\n",
        "        # Set ticks and labels\n",
        "        axs[head].set_xticks(range(len(tokens)))\n",
        "        axs[head].set_yticks(range(len(tokens)))\n",
        "        axs[head].set_xticklabels(tokens, rotation=90, ha='right')\n",
        "        axs[head].set_yticklabels(tokens)\n",
        "        axs[head].set_title(f'Head {head+1}')\n",
        "\n",
        "    plt.colorbar(img, ax=axs)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5NyZaVMVnDeK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_encoder_decoder_attentions_maps(input, output_seq, attentions):\n",
        "    in_tokens = tokenizer.tokenize(input)\n",
        "    #out_tokens = tokenizer.tokenize(output)\n",
        "    out_tokens = [tokenizer.decode(id) for id in output_seq[0][1:-1]]\n",
        "\n",
        "    n_heads = 5\n",
        "\n",
        "    attention = attentions[4][0]\n",
        "    fig, axs = plt.subplots(1, n_heads, figsize=(len(in_tokens)*2,len(out_tokens)//1.7), sharey=True)\n",
        "\n",
        "    for head in range(n_heads):\n",
        "        att_matrix = attention[head,2:-1,1:-1].detach().numpy()\n",
        "        img = axs[head].imshow(att_matrix, cmap='Blues')\n",
        "\n",
        "        # Set ticks and labels\n",
        "        axs[head].set_xticks(range(len(in_tokens)))\n",
        "        axs[head].set_yticks(range(len(out_tokens)))\n",
        "        axs[head].set_xticklabels(in_tokens, rotation=90, ha='right')\n",
        "        axs[head].set_yticklabels(out_tokens)\n",
        "        axs[head].set_title(f'Head {head+1}')\n",
        "\n",
        "    plt.colorbar(img, ax=axs)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "slyJdPLTnItT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_softmax(logits, temperature: float = 1):\n",
        "    scores = logits[0,0,:].clone().detach()\n",
        "    if (temperature > 0):\n",
        "        scores /= temperature\n",
        "        softmax = torch.nn.functional.softmax(scores, dim=-1,)\n",
        "        top_probs, top_indices = torch.topk(softmax, k=10)\n",
        "    else:\n",
        "        softmax = torch.nn.functional.softmax(scores, dim=-1,)\n",
        "        top_probs, top_indices = torch.topk(softmax, k=10)\n",
        "        top_probs[0] = 1.0\n",
        "        top_probs[1:] = 0\n",
        "\n",
        "\n",
        "    print()\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.bar([tokenizer.decode(int(ind)) for ind in top_indices], top_probs, color='skyblue')\n",
        "    plt.xlabel('Probability')\n",
        "    plt.ylabel('Token')\n",
        "    plt.title('Top 10 Softmax Scores for Output Tokens')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RpJPwrZ2naAO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_outputs(temperature: float = 1, n_samples: int = 5):\n",
        "    print(f'\\n Exemplos de frases amostradas com temperatura {temperature}\\n')\n",
        "    for i in range(n_samples):\n",
        "        output_seq = run_model(Input, temperature=temperature, output_hidden_states=False)\n",
        "        print(f'{i+1}) ' + tokenizer.decode(output_seq[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "pxw2zsnxnccf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(text: str, temperature: float = 0.0, output_hidden_states: bool = True):\n",
        "    #Tokeniza o texto\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    # Computa os ids dos tokens\n",
        "    inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "    with torch.no_grad():\n",
        "    #Realiza a inferência do modelo e recupera os estados internos do modelo\n",
        "        if temperature > 0:\n",
        "            output_seq = model.generate(**inputs, do_sample=True, temperature=temperature, max_length=50)#, forced_bos_token_id=tokenizer.lang_code_to_id['pt_XX'])\n",
        "            if output_hidden_states:\n",
        "                outputs = model(**inputs, decoder_input_ids=output_seq, output_hidden_states=True, output_attentions=True)\n",
        "        else:\n",
        "            output_seq = model.generate(**inputs, max_length=50)#, forced_bos_token_id=tokenizer.lang_code_to_id['pt_XX'])\n",
        "            if output_hidden_states:\n",
        "                outputs = model(**inputs, decoder_input_ids=output_seq, output_hidden_states=True, output_attentions=True)\n",
        "\n",
        "     # Retorna o resultado da camada linear de saída e os estados internos\n",
        "    if output_hidden_states:\n",
        "        return output_seq, outputs.logits, outputs.encoder_hidden_states, outputs.decoder_hidden_states, outputs.encoder_attentions, outputs.cross_attentions\n",
        "    else:\n",
        "        return output_seq\n"
      ],
      "metadata": {
        "id": "9UCWWtfpnj-p"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Entendendo o Transformer"
      ],
      "metadata": {
        "id": "3GLLPlFSnzhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*A arquitetura original do transformer é um modelo auto-encoder seq2seq baseado inteiramente em self-attention. De forma mais detalhada, o transformer é um modelo que:\n",
        "\n",
        "1-Converte uma entrada para uma representação vetorial comprimida e então gera uma saída, do mesmo tipo da entrada, a partir da representação (auto-encoder);\n",
        "\n",
        "2-Aceita como entrada uma sequência de dados e produz como saída uma sequência de dados, onde a ordem dos itens na sequência é importante, como em textos e séries temporais (seq2seq);\n",
        "3-Utiliza somente o mecanismo de atenção (particularmente o self-attention) para tratar a relação temporal entre os itens das sequências de entrada e saída."
      ],
      "metadata": {
        "id": "Shx8xSApoIME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Cn3fNFiioQxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#* Processamento passo-a-passo"
      ],
      "metadata": {
        "id": "zzT8DEvBorHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##O passo-a-passo do funcionamento de um transformer. O modelo estudado será o OPUS-mt English to Portuguese, o qual é um transformer encoder-decoder  treinado para realizar a tradução de textos do inglês para o português.\n",
        "\n"
      ],
      "metadata": {
        "id": "aidcaSP0pE3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A arquitetura do transformer possui dois componentes principais: o encoder e o decoder. Ambos são formados por blocos de processamento empilhados, os quais realizam as mesmas operações, mas mantém um conjunto de parâmetros próprio.\n",
        "\n",
        "##1-O encoder gera uma representação vetorial da sentença de entrada. Ele é composto por uma camada de self-attention, o qual permite a modificação do embedding de cada token com base nos embeddings dos demais tokens, e uma camada linear, que modifica cada token independentemente.\n",
        "\n",
        "##2-O decoder gera a sentença de saída 'palavra por palavra' utilizando a representação criada pelo encoder. Ele possui as mesmas duas camadas que o encoder e uma camada extra entre elas que realiza a atenção entre os tokens do decoder com os tokens do último bloco do encoder, permitindo que a saída agregue informações da entrada."
      ],
      "metadata": {
        "id": "6zubPmqcpNaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Primeiramente vamos selecionar um texto para tradução, o qual será a entrada do modelo.\n",
        "Input = \"A sociedade Academica é Constantemente avaliada\" # @param @type {type: \"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RxYdDLhsWVuk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Tokens"
      ],
      "metadata": {
        "id": "hStNlq3uqrX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Para que o transformer possa processar dados textuais, os mesmos devem ser transformados em vetores numéricos. Para isso, a entrada é dividida em tokens com base em um dicionário previamente definido.\n",
        "\n",
        "#Os dicionários costumam ser únicos para cada modelo e as palavras ou sub-palavras presentes neles são escolhidas pela taxa de ocorrência das mesmas nos dados de treinamento. Os dicionários mapeiam cada token (palavra ou sub-palavra) para um índice (token id).\n",
        "\n"
      ],
      "metadata": {
        "id": "LAIeuRLKqwIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_colored_tokens(Input)"
      ],
      "metadata": {
        "id": "5SURWYzRqvYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Embeddings"
      ],
      "metadata": {
        "id": "GfT-PTdMrIvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Para permitir que os transformers representem melhor a informação simbólica que cada token id representa são utilizados embeddings. Embeddings são vetores de alta dimensão que representam uma informação simbólica de forma distribuída. Ao operarem sobre embeddings ao invés de token ids, os modelos possuem mais liberdade para focar e/ou alterar partes específicas da informação de um token.\n",
        "\n",
        "##A camada de embedding inicial de um transformer consiste um uma tabela simples, onde cada linha é um vetor de embedding. Dessa forma, o modelo simplesmente seleciona as linhas correspondentes aos token ids de entrada e forma uma sequência de embeddings para entrada do encoder ou decoder.\n",
        "\n",
        "##Vamos visualizar abaixo uma parte dos embeddings de entrada do encoder. O valor de cada índice do vetor de embedding será representado por um retângulo com cor proporcional a seu valor. Pode-se reparar como a informação que um token carrega é distribuída pelo vetor, tendo valores positivos e negativos em cada posição, indicando a importância da dimensão para a interpretação do token.\n",
        "\n"
      ],
      "metadata": {
        "id": "E8GPWONSrTGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Executando o modelo\n",
        "output_seq, logits, encoder_hidden_states, decoder_hidden_states, attentions, decoder_attentions = run_model(Input)\n",
        "\n",
        "# Vizualizando os embeddings de entrada e internos\n",
        "print_input_embeddings(Input, encoder_hidden_states)\n"
      ],
      "metadata": {
        "id": "HVnxGntwrfrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Atenção"
      ],
      "metadata": {
        "id": "JQfTLfGmrmCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##* O processamento dos embeddings é feito com o mecanismo de self-attention. O self-attention opera como uma função sobre uma sequência de embeddings de entrada e gera uma sequência de mesmo tamanho como saída. Cada elemento da saída é uma soma ponderada de todos os elementos da entrada. Dessa forma, dado uma entrada  [x1,x2,...,xn] , a saída  yi  é:\n",
        "\n",
        "                                   yi=∑jwijxj\n",
        "\n",
        "##* A diferença da self-attention, está no fato que os pesos  wij  não são parâmetros do modelo, mas obtidos por uma função entre  xi  e  xj :\n",
        "\n",
        "                                   yi=∑jf(xi,xj)xj\n",
        "\n",
        "##* Dessa forma, o valor do vetor na posição  i  da sequência de saída depende da relação do vetor de entrada na mesma posição com todos os vetores da sequência.\n",
        "\n",
        "##* Os valores de atenção podem ser agrupados em uma matriz com todas as combinações i e j para  f(xi,xj) , formando uma matrix  n  x  n . Abaixo pode-se visualizar algumas matrizes de atenção em uma das camadas do encoder do modelo. Nela podemos ver como o peso da soma ponderada é distribuído pelos diferentes tokens para formar a representação de saída de cada um."
      ],
      "metadata": {
        "id": "bmgKbg14rwQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_encoder_attentions_maps(Input, attentions)"
      ],
      "metadata": {
        "id": "uM0LUEOrrjFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##* No decoder do modelo é inserido um mecanismo de atenção extra que permite os tokens do decoder (no nosso exemplo são os tokens da frase traduzida para o português) prestarem atenção nos tokens do encoder (frase em inglês). O príncipio de funcionamento é o mesmo descrito anteriormente, porém agora temos duas sequências de entrada gerando uma sequência de saída. Sendo a sequência do encoder  [z1,z2,...,zn]  e a sequência do decoder  [x1,x2,...,xn] , a saída na posição i é dada por:\n",
        "\n",
        "                                 yi=∑jf(xi,zj)zj\n",
        "\n",
        "##*Dessa forma, a saída é uma soma ponderada dos tokens do encoder, onde os pesos de ponderação dependem dos tokens do decoder e encoder.\n",
        "\n",
        "##*Abaixo pode-se visualizar algumas matrizes de atenção entre o decoder e encoder para o modelo.\n",
        "\n"
      ],
      "metadata": {
        "id": "5shfw-Qdtv7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_encoder_decoder_attentions_maps(Input, output_seq, decoder_attentions)"
      ],
      "metadata": {
        "id": "Qxygp5gXuDo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# * Saída"
      ],
      "metadata": {
        "id": "Vlei-uVguX4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##* Após a última camada do decoder o modelo deve decidir qual o próximo token da sequência de saída. Para isso, uma última camada linear é utilizada para projetar o embedding da saída do decoder para um vetor com o tamanho do vocabulário. Cada posição do vetor indica um valor de importância do token correspondente no vocabulário. Esse vetor é chamado de logitos, se referindo a predições não-normalizadas do modelo.\n",
        "\n",
        "##* Após isso, é necessário normalizar os valores de logitos para uma distribuição de probabilidade. Isso é feito utilizando a função softmax, a qual limita cada valor  xi  entre 0 e 1 e faz a soma total do vetor ser 1:\n",
        "\n",
        "                  exi∑jexj"
      ],
      "metadata": {
        "id": "N3iBKp8Gufif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Com isso, temos na saída do modelo uma distribuição de probabilidades que pode ser utilizada para amostrar o próximo token de saída. A forma mais direta é escolher aquele de maior probabilidade, porém é possível também realizar uma amostragem ponderada, onde o vetor da saída define a probabilidade de um determinado token ser amostrado, ou seja, escolhido como o próximo da sequência de saída. Assim, os tokens mais prováveis de continuarem a sequência de saída terão maior probabilidade de serem selecionados."
      ],
      "metadata": {
        "id": "aqce0Yi7vov0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_dim = logits.size()\n",
        "print(f\"Dimensões da saída do modelo: {list(out_dim)}\")\n",
        "print(f\"Batch: {out_dim[0]} - Tamanho da Sequência: {out_dim[1]} - Tamanho do Vocabulário: {out_dim[2]}\")"
      ],
      "metadata": {
        "id": "C8iGQSy0vuOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Utilizando a distribuição de probabilidades da saída do modelo, pode-se escolher a sequência de tokens mais provável\n",
        "\n"
      ],
      "metadata": {
        "id": "ik2zBVpov77M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(output_seq[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "69EWBo_SwAuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#* A sequência de saída do modelo pode ser escolhida por uma amostragem ponderada baseada no vetor de distribuição de probabilidades do decoder. Porém, devido ao método comum de treinamento dos modelos, os mesmos tendem a atribuir a maior parte da probabilidade a somente alguns tokens. Com isso, ao se amostrar diversas sequências de saída para uma mesma entrada, a variabilidade costuma ser baixa.\n",
        "\n",
        "##Para aumentar a variabilidade na amostragem da sequência de saída, ou seja, permitir o modelo ter mais liberdade para gerar textos diversificados, é introduzido o parâmetro temperatura  τ :\n",
        "\n",
        "                                exi/τ∑jexj/τ\n",
        "\n",
        "##Como a soma total dos itens após o softmax é sempre igual a 1, ao aumentar o valor da temperatura, a probabilidade é mais distribuída entre os itens, aproximando a distribuição a uma distribuição uniforme. Consequentemente, ao diminuir o valor da temperatura ( 0<τ<1 ), a probablidade dos itens mais relevantes é aumentada e as dos demais itens aproxima de zero. No limite onde  τ=0 , a distribuição é categórica e somente um elemento recebe 100% da probabilidade"
      ],
      "metadata": {
        "id": "XJFDOVZ4wOOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#* Abaixo pode-se visualizar a distribuição de probabilidade para a primeira palavra na sequência de saída do decoder. O gráfico apresenta somente as 10 palavras mais prováveis e a direita pode-se selecionar um valor de temperatura e então executar a célula para ver a mudança da distribuição.\n",
        "\n"
      ],
      "metadata": {
        "id": "9crxTiYGwerE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown A execução deste código é lenta devido ao tamanho do modelo e vetores utilizados\n",
        "temperatura = 1 # @param @type {type: \"slider\", min:0, max:8, step: 0.1}\n",
        "\n",
        "plot_softmax(logits, temperatura)"
      ],
      "metadata": {
        "id": "gn2cM4R9wkZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##* Por fim podemos amostrar diferentes sequências de saída com a temperatura selecionada. Quando a temperatura é zero, todas as saídas geradas serão iguais, pois o processo se torna determinístico. Conforme a temperatura é aumentada (pelo controle da célula anterior), mais diversificada se tornam as frases, porém se a temperatura for muito alta, as frases se aproximam cada vez mais de serem aleatórias.\n",
        "\n"
      ],
      "metadata": {
        "id": "syOtaV0xwtyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_outputs(float(temperatura))"
      ],
      "metadata": {
        "id": "zJGzAOLjw1SU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}