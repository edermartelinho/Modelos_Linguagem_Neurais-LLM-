{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "Od4qFBfdb3mC"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edermartelinho/Modelos_Linguagem_Neurais-LLM-/blob/main/M%C3%A9tricas_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Métricas de Avaliação para Modelos de Linguagem"
      ],
      "metadata": {
        "id": "vfy69J6Gz4KV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelos de linguagem podem ser aplicados em diversas tarefas de processamento de linguagem natural. Para avaliar o desempenho de tais modelos e comparar suas performances nessas tarefas, diversas métricas são utilizadas. As métricas permitem quantificar objetivamente o desempenho na tarefa e fornece uma base comum de comparação. Além disso, análises mais detalhadas dos casos de erro mais extremos podem fornecer informações úteis para implementação de melhorias nas arquiteturas.\n",
        "\n",
        "As métricas de avaliação necessitam também de um conjunto de dados para teste. O modelo em si, sem o fornecimento de dados de entrada e uma saída esperada, não fornece informação que permite julgar o seu desempenho, dessa forma conjuntos de dados são criados para a avaliação das tarefas. Como se fosse uma prova que aplicada para várias pessoas e corrigida pelo mesmo grupo de avaliadores, garante que a performance de todos seja comparável."
      ],
      "metadata": {
        "id": "tbce2qPCz9P1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependências"
      ],
      "metadata": {
        "id": "Od4qFBfdb3mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers evaluate sentencepiece sacremoses sacrebleu"
      ],
      "metadata": {
        "id": "pkePTmVXLcvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "transformers.logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "UGN7XEVPhURk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluator"
      ],
      "metadata": {
        "id": "DtW-mdGqb70K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "import random"
      ],
      "metadata": {
        "id": "XaNnxymVJxl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perplexidade"
      ],
      "metadata": {
        "id": "StvyBg_Cczhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As métricas intrínsicas avaliam o desempenho de modelos de linguagem em sua tarefa básica de treinamento, ou seja, capturar a distribuição estatística dos dados. Em modelos de linguagem auto-regressivo (GPT, LaMDA, PaLM) o modelamento estatístico busca definir a probabilidade do próximo token da sequência, atribuindo alta probabilidade aos tokens mais prováveis.\n",
        "\n",
        "A medida de perplexidade exprime o nível de incerteza que o modelo possui ao definir a distribuição de probabilidade do próximo token dado uma sequência anterior. Assim, quanto maior a perplexidade mais incerto o modelo é sobre sua predição, distribuindo mais a probabilidade entre diversos tokens, ou seja, ao invés de ter somente um bom candidato para continuação da sequência são preditos múltiplos candidatos. É importante notar nessa definição que a perplexidade é somente uma medida de confiança do modelo quanto sua predição, mas **não** da sua acurácia. Formalmente a perplexidade é dada por:\n",
        "\n",
        "$$\n",
        "e^{- \\frac{1}{N} \\sum_{1}^{N} ln\\, p_{\\theta}(x_i|x_{i-1},...,x_{0})} = \\prod_{1}^{N} p_{\\theta}(x_i|x_{i-1},...,x_{0})^{-1/N}\n",
        "$$\n",
        "\n",
        "Onde $x_i$ é o token de maior probabilidade para sequência, $p_\\theta$ é a probabilidade de um token dado uma sequência anterior e $N$ é o total de tokens na sequência.\n",
        "\n",
        "Abaixo iremos testar três modelos com a mesma arquitetura, mas número de parâmetros diferentes. Para isso, utilizamos o dataset [IMDB]() que contem reviews de filmes e o módulo `evaluate` do Hugging Face para computar automaticamente a métrica.\n",
        "\n"
      ],
      "metadata": {
        "id": "p_TlOdSoWdEl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvcZ-MAI3b6Y"
      },
      "outputs": [],
      "source": [
        "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
        "dataset_texts = load_dataset(\"imdb\", split=\"test\").shuffle()[\"text\"]\n",
        "\n",
        "# Seleciona um subconjunto aleatório do dataset\n",
        "n = len(dataset_texts)\n",
        "subset = random.choices(range(n), k=500)\n",
        "#dataset_texts = dataset_texts.select(subset)\n",
        "dataset_texts = [dataset_texts[i] for i in subset]\n",
        "\n",
        "results_70m = perplexity.compute(model_id='EleutherAI/pythia-70m-deduped',\n",
        "                             predictions=dataset_texts,\n",
        "                             batch_size=8)\n",
        "\n",
        "\n",
        "results_160m = perplexity.compute(model_id='EleutherAI/pythia-160m-deduped',\n",
        "                             predictions=dataset_texts,\n",
        "                             batch_size=8)\n",
        "\n",
        "results_410m = perplexity.compute(model_id='EleutherAI/pythia-410m-deduped',\n",
        "                             predictions=dataset_texts,\n",
        "                             batch_size=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abaixo podemos ver os resultados de perplexidade para cada modelo."
      ],
      "metadata": {
        "id": "01tCs6LZ71EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print('Pythia 70M: ' + results_70m['mean_perplexity'])\n",
        "#print('Pythia 160M: ' + results_160m['mean_perplexity'])\n",
        "#print('Pythia 410M: ' + results_410m['mean_perplexity'])\n",
        "print(f'Pythia 70M: {results_70m[\"mean_perplexity\"]}')\n",
        "print(f'Pythia 160M: {results_160m[\"mean_perplexity\"]}')\n",
        "print(f'Pythia 410M: {results_410m[\"mean_perplexity\"]}')"
      ],
      "metadata": {
        "id": "99WoebOqoqdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQuAD V2"
      ],
      "metadata": {
        "id": "5703CGIAc1j9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Stanford Question Answering Dataset (SQuAD) é um conjunto de dados de triplas contendo um contexto, uma pergunta e uma resposta voltado para a avaliação da capacidade de compreensão de linguagem. Em cada tripla, a resposta é um trecho do texto de contexto que responde a pergunta. A tarefa do modelo é, a partir da sequência de entrada composta pelo contexto e pergunta, gerar a posição inicial e final do trecho do contexto que responde a pergunta. Adicionalmente, no SQuAD v2 existem perguntas que não podem ser respondidas a partir do contexto fornecido e o modelo deve ser capaz de indicar que não há resposta para pergunta.\n",
        "\n",
        "O desempenho do modelo no dataset é medido com a métrica F1, a qual indica de maneira geral a qualidade do modelo em acertar o trecho da resposta. Pode-se também analisar a métrica F1 separadamente para os dados que possuem resposta e os que não podem ser respondidos."
      ],
      "metadata": {
        "id": "0y2Q60Tsh9dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qna_model_tiny = 'deepset/tinyroberta-squad2'\n",
        "qna_model_base = 'deepset/roberta-base-squad2'\n",
        "qna_model_large = 'deepset/roberta-large-squad2'\n",
        "\n",
        "squad_v2_dataset = load_dataset('squad_v2', split='validation')\n",
        "\n",
        "# Seleciona um subconjunto aleatório do dataset\n",
        "n = len(squad_v2_dataset)\n",
        "subset = random.choices(range(n), k=1000)\n",
        "squad_v2_dataset = squad_v2_dataset.select(subset)"
      ],
      "metadata": {
        "id": "3-_J0anZpqOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qna_eval = evaluate.evaluator('question-answering')\n",
        "\n",
        "results_tiny = qna_eval.compute(model_or_pipeline=qna_model_tiny,\n",
        "             data=squad_v2_dataset,\n",
        "             squad_v2_format=True,\n",
        "             metric='squad_v2')\n",
        "\n",
        "results_base = qna_eval.compute(model_or_pipeline=qna_model_base,\n",
        "             data=squad_v2_dataset,\n",
        "             squad_v2_format=True,\n",
        "             metric='squad_v2')\n",
        "\n",
        "results_large = qna_eval.compute(model_or_pipeline=qna_model_large,\n",
        "             data=squad_v2_dataset,\n",
        "             squad_v2_format=True,\n",
        "             metric='squad_v2')"
      ],
      "metadata": {
        "id": "xFlllzGobYLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_tiny['f1'], results_tiny['HasAns_f1'], results_tiny['NoAns_f1'])\n",
        "print(results_base['f1'], results_base['HasAns_f1'], results_base['NoAns_f1'])\n",
        "print(results_large['f1'], results_large['HasAns_f1'], results_large['NoAns_f1'])"
      ],
      "metadata": {
        "id": "yJDkmbbJthO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU"
      ],
      "metadata": {
        "id": "Dov6rhznc-JP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Medir o desempenho de modelos de tradução é um processo complexo, pois envolve a avaliação de aspectos subjetivos como compreensão, fidelidade e fluência da tradução dado que não existe uma única tradução possível para um texto. A métrica BLEU é uma forma automatizada de comparar a tradução gerada por um modelo de linguagem com uma ou, preferencialmente, múltiplas traduções de referência feitas por profissionais. O valor BLEU exprime o quão próximo a tradução gerada pelo modelo está das traduções de referência. Dessa forma, quanto maior o seu valor, melhor a tradução. É importante destacar que atingir o valor máximo (100) de pontuação é extremamente improvável, pois iria requerer que todas as traduções fossem idênticas a uma referência. Além disso, ao utilizar poucas referências de tradução para comparação o resultado tende a ter valores menores"
      ],
      "metadata": {
        "id": "0FcPVKd2dKI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_pt_translation_dataset = load_dataset('opus_books', 'en-pt', split='train')\n",
        "\n",
        "# Seleciona um subconjunto aleatório do dataset\n",
        "n = len(en_pt_translation_dataset)\n",
        "subset = random.choices(range(n), k=500)\n",
        "en_pt_translation_dataset = en_pt_translation_dataset.select(subset)\n",
        "\n",
        "english_texts = [translation_pair['en']\n",
        "                 for translation_pair in en_pt_translation_dataset['translation']]\n",
        "\n",
        "portuguese_texts = [translation_pair['pt']\n",
        "                 for translation_pair in en_pt_translation_dataset['translation']]\n",
        "\n",
        "en_pt_translation_dataset = en_pt_translation_dataset.add_column('english', english_texts)\n",
        "en_pt_translation_dataset = en_pt_translation_dataset.add_column('portuguese', portuguese_texts)"
      ],
      "metadata": {
        "id": "gqKYbJx0u6WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translation_model_opus = 'Helsinki-NLP/opus-mt-tc-big-en-pt'\n",
        "\n",
        "sacrebleu_eval = evaluate.evaluator('translation')\n",
        "\n",
        "results_opus = sacrebleu_eval.compute(model_or_pipeline=translation_model_opus,\n",
        "             data=en_pt_translation_dataset,\n",
        "             input_column='english',\n",
        "             label_column='portuguese',\n",
        "             metric='sacrebleu')"
      ],
      "metadata": {
        "id": "k4jE7WuadAAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_opus['score'])"
      ],
      "metadata": {
        "id": "pDqe9gG4zK8l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}