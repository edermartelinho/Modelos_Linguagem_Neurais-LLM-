{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgRsdJ/xgjXSGj7BsvpU4M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edermartelinho/Modelos_Linguagem_Neurais-LLM-/blob/main/Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#* Bibliotecas"
      ],
      "metadata": {
        "id": "0mDh5M7i5HR5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU4eKhTk4lHx"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install -U mteb\n",
        "!pip install pysentimiento\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import KeyedVectors\n",
        "from huggingface_hub import hf_hub_download\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer,util\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.express as px\n",
        "from pysentimiento import create_analyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#* Similaridade de cosseno"
      ],
      "metadata": {
        "id": "JzeNL_d66hD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exemplos de incorporação de palavras\n",
        "\n",
        "word_embeddings = {\n",
        "    \"king\": np.array([0.1, 0.2, 0.3]),\n",
        "    \"queen\": np.array([0.15, 0.25, 0.35]),\n",
        "    \"man\": np.array([0.4, 0.5, 0.6]),\n",
        "    \"woman\": np.array([0.45, 0.55, 0.65]),\n",
        "    \"apple\": np.array([0.7, 0.8, 0.9]),\n",
        "    \"orange\": np.array([0.75, 0.85, 0.95]),\n",
        "    \"magesty\":np.array([101,687,5923])\n",
        "}\n",
        "\n",
        "def cosine_sim(word1, word2):\n",
        "  return cosine_similarity([word_embeddings[\"king\"]], [word_embeddings[\"queen\"]])[0][0]\n",
        "\n",
        "similarity_King_queen = cosine_sim(\"King\", \"queen\")\n",
        "print(f\"Similaridade de cosseno entre'King' e 'queen': {similarity_King_queen}\")\n"
      ],
      "metadata": {
        "id": "Y29nRmYc6v5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##* A similaridade de coseno indica que os embedding 'King' e 'Queen' tem um grau de similaridade de 0.9974149030430577, enquanto 'man' e 'Woman' tem uma similaridade de 0.9998949782076886 e 'apple' e 'orange' apresenta uma similaridade de 0.9999823267952279, esta analise mostra que as palavras estão muito proxima em um espaco vetorial"
      ],
      "metadata": {
        "id": "n7Cyu0V3-FpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = KeyedVectors.load_word2vec_format(hf_hub_download(repo_id=\"Word2vec/wikipedia2vec_ptwiki_20180420_100d\", filename=\"ptwiki_20180420_100d.txt\"))\n",
        "#model.most_similar(\"sua_palavra\")\n",
        "\n",
        "#Supondo que você baixou o modelo corretamente e o carregou em uma variável chamada 'modelo'\n",
        "\n",
        "similar_words = model.most_similar('carro', topn=5)  # Obtenha as 5 principais palavras semelhantes\n",
        "print(f\"Words similar to 'carro': {similar_words}\")\n",
        "\n",
        "result = model.most_similar(positive=['carro', 'fusca'], negative=['trator'], topn=1)\n",
        "print(f\"carro - trator + fusca = {result[0][0]}\")  # Imprima a palavra mais semelhante\n",
        "\n"
      ],
      "metadata": {
        "id": "DTT3HZjj-S2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*  word2vec\n",
        "É uma tecnica de aprendizado de unidades de representações distribuidas proposta por Mikolov et al(2013)que tem como objetivo capturar a semântica e a relação entre unidades de representação em um corpus, aprendendo embeddings estaticos para cada palavra presente no vocabulario de treino(Jurasfsky;Martin,2023) a ideia é que palavras que ocorrem em contexto semelhantes tenham significados semelhantes.\n",
        "\n"
      ],
      "metadata": {
        "id": "nrOicuYKBepL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#* Gensim\n",
        "O gensim é uma biblioteca python para modelagem de topicos, indexação de documentos e recuperação de similaridade com grandes corporas.\n",
        "\n",
        "Ao usar o modelo: from gensim.models import keyedVectors percebi que a maior similaridade no top 5 foi 'caminhão' com similaridade de 0.8685649633407593, percebo tambem que quando coloco trator como negativo o modelo me sugere um carro mais moderno"
      ],
      "metadata": {
        "id": "dld7b4EJCvAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('mteb-pt/average_pt_nilc_glove_s1000')\n",
        "# Função para calcular similaridade de cosseno\n",
        "def cosine_similarity(v1, v2):\n",
        "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "# a analogia: “mar” está para “baleia” assim como “passaro” está para ?\n",
        "#mar = modelo['mar']\n",
        "#baleia = modelo['baleia']\n",
        "#passaro = modelo['passaro']\n",
        "\n",
        "mar = model.encode(['mar'])[0] # Use .encode() e obtenha o primeiro elemento\n",
        "baleia = model.encode(['baleia'])[0] # Use .encode() e obtennha o segundo elemento\n",
        "passaro = model.encode(['passaro'])[0] # Use .encode() e obtenha o primei elemento\n",
        "\n",
        "\n",
        "\n",
        "result_vector = passaro - mar + baleia\n",
        "\n",
        "vocabulary = ['aguia','pombo','tubarão','gaivota']\n",
        "\n",
        "\n",
        "max_similarity = -1\n",
        "most_similar_word = ''\n",
        "for word in vocabulary:\n",
        "\n",
        "       similarity = util.cos_sim(result_vector, model.encode([word])[0]).item()\n",
        "       if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            most_similar_word = word\n",
        "\n",
        "print(f\"'mar' is to 'baleia' as 'passaro' is to '{most_similar_word}'\")"
      ],
      "metadata": {
        "id": "0T8Ag-3wDj6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# * GLOve\n",
        "##O GLOve adota uma perspectiva global, levando em consideração a contagem de coocorência palavra-palavra em um corpus. Essa abordagem permite que o glove capture informações de relação semântica e sintática entre as palavras.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mcVB7rQ1PwKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-en-pt-cased\")\n",
        "model = AutoModel.from_pretrained(\"Geotrend/bert-base-en-pt-cased\")\n",
        "\n",
        "\n",
        "print(f\"Using model: {tokenizer}\")\n"
      ],
      "metadata": {
        "id": "bJ0xfAuAQcIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Importei um tokenizador, define a lista de palavras para explorar e tokenizei,foi extraido o ultimo estado oculto e calculada a media para obter uma unica incorporação para cada palavr da lista, calculei a similaridade de coseno, aplyquei o pca e plotei os imbedin, define algumas sentenças, tokenizeie gerei embeddings, extrai a media do ultimo estado oculto e plotei embeddings de frasese em 2d usando PCA, tambem usei as mesmas sentenças para fazer analise de sentimento mostrando o resultado executando a celula."
      ],
      "metadata": {
        "id": "xSAOBLEjRedv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"mar\", \"maré\", \"maremoto\", \"marrevolto\", \"carro\",\"caminhão\",\"caminhoneiro\", \"fusca\", \"carroça\", \"fiat147\"]\n",
        "\n",
        "# Tokenizar e gerar embeddings\n",
        "inputs = tokenizer(words, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Extraia o último estado oculto e calcule a média para obter uma única incorporação para cada palavra\n",
        "embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "print(\"Embeddings shape:\", embeddings.shape)"
      ],
      "metadata": {
        "id": "t-Tzvm97RoKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "print(\"Cosine Similarity Matrix:\\n\", similarity_matrix)\n"
      ],
      "metadata": {
        "id": "7PcsnK7gSmab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*A Análise de Componentes Principais (ACP) ou Principal Component Analysis (PCA)\n",
        "\n",
        "##É um procedimento matemático que utiliza uma transformação ortogonal (ortogonalização de vetores) para converter um conjunto de observações de variáveis possivelmente correlacionadas num conjunto de valores de variáveis linearmente não correlacionadas chamadas de componentes principais"
      ],
      "metadata": {
        "id": "lhi3qHS_W062"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando PCA\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Traçando os embeddings\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, word in enumerate(words):\n",
        "    x, y = embeddings_2d[i]\n",
        "    plt.scatter(x, y)\n",
        "    plt.text(x + 0.02, y + 0.02, word, fontsize=12)\n",
        "\n",
        "plt.title(\"2D Visualization of Word Embeddings\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "etcd66V2S1Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Incorporação de vizinho estocástico distribuída em T.\n",
        "\n",
        "#t-SNE  é uma ferramenta para visualizar dados de alta dimensão. Ele converte semelhanças entre pontos de dados em probabilidades conjuntas e tenta minimizar a divergência de Kullback-Leibler entre as probabilidades conjuntas da incorporação de"
      ],
      "metadata": {
        "id": "gsh04uVjYTH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " tsne = TSNE(n_components=3, perplexity= 5 )\n",
        " projections = tsne.fit_transform(embeddings)\n",
        "\n",
        " fig = px.scatter_3d(projections, x=0, y=1, z = 2, color=words)\n",
        " fig.show()"
      ],
      "metadata": {
        "id": "mISCVdYlTyBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defina frases com semelhança semântica\n",
        "sentences = [\"As eleiçoes foram acirradas entre os partidos.\",\n",
        "             \"Alguns partidos reelegeram seus prefeitos.\",\n",
        "             \"A abstenção nas eleições gerou debates\",\n",
        "             \"O derretimento das calotas polares é preocupante.\",\n",
        "             \"O degelo muda o clima.\",\n",
        "             \"Efeitos do aquecimento global.\"\n",
        "             ]\n",
        "\n",
        "# Tokenizar e gerar embeddings\n",
        "sentence_inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "with torch.no_grad():\n",
        "    sentence_outputs = model(**sentence_inputs)\n",
        "\n",
        "#Extrai a média do último estado oculto\n",
        "sentence_embeddings = sentence_outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "# Visualize incorporações de frases em 2D usando PCA\n",
        "sentence_embeddings_2d = pca.fit_transform(sentence_embeddings)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, sentence in enumerate(sentences):\n",
        "    x, y = sentence_embeddings_2d[i]\n",
        "    plt.scatter(x, y)\n",
        "    plt.text(x + 0.02, y + 0.02, sentence, fontsize=10)\n",
        "\n",
        "plt.title(\"2D Visualization of Sentence Embeddings\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "eF4OdQfQY9wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " tsne = TSNE(n_components=3, perplexity= 5 )\n",
        " projections = tsne.fit_transform(sentence_embeddings)\n",
        "\n",
        " num_points = projections.shape[0]  # Get the number of data points\n",
        " colors =sentences[:num_points]  # Slice the words list to match the numbe\n",
        "\n",
        " fig = px.scatter_3d(projections, x=0, y=1, z = 2, color=colors)\n",
        " fig.show()"
      ],
      "metadata": {
        "id": "RhitQK9_eX6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Analise de Sentimento\n",
        "\n",
        "#A análise de sentimentos tem várias aplicações práticas, desde o monitoramento de marcas até a análise de comentários de clientes."
      ],
      "metadata": {
        "id": "cOvkuz1xcwdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = create_analyzer(task=\"sentiment\", lang=\"pt\")\n",
        "# Chame o método 'predict' entre parênteses e passe a lista de sentenças\n",
        "for sentence in sentences:\n",
        "    sentiment = analyzer.predict(sentence)\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Sentiment: {sentiment.output}\")  # Access the sentiment output\n",
        "\n",
        "# retorna AnalyzerOutput(saída=POS, probas={POS: 0,998, NEG: 0,002, NEU: 0,000})\n"
      ],
      "metadata": {
        "id": "EThJaJcUbMJ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}