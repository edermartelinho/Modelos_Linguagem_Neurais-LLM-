{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edermartelinho/Modelos_Linguagem_Neurais-LLM-/blob/main/Toxicidade_e_Vi%C3%A9s.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avaliando Toxicidade e Vieses em LLMs"
      ],
      "metadata": {
        "id": "_uP_wCrZVPGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apesar das grandes capacidades emergentes dos LLMs, esses ainda são modelos estatíscos que reproduzem os padrões de texto vistos em treinamento, inclusive aqueles que podem ser vistos como ofensivo, obsceno e/ou discriminatório. Com isso, ao utilizar esses modelos em aplicações e produtos, os mesmos podem disseminar linguagem ofensiva, desinformação e vieses aos seus usuários.\n",
        "\n",
        "É importante lembrar que LLMs apenas reproduzem textos com características (impressionantemente) semelhantes a textos humanos, porém não possuem nenhum entendimento real do que estão comunicando."
      ],
      "metadata": {
        "id": "FYGHfxF_VV1s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh3XOMUl55cx"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers evaluate datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import pipeline\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "\n",
        "# Verificando se temos acesso a uma GPU\n",
        "if torch.cuda.is_available():\n",
        "   dev = \"cuda:0\"\n",
        "else:\n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)"
      ],
      "metadata": {
        "id": "zgfg8phPCEcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Toxicidade"
      ],
      "metadata": {
        "id": "MwF3xnZEAY1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma das características indesejadas nos textos gerados por LLMs é a toxicidade. A definição formal do termo ainda é discutida na área de Machine Learning, Ética e Jurídica por depender de diferentes contextos sociais, culturais e morais. De maneira simples, um texto tóxico é aquele que \"contém um comentário rude, desrepeitoso ou insensato que faria uma pessoa sair de uma conversa\" [[Perspective AI](https://perspectiveapi.com/)]."
      ],
      "metadata": {
        "id": "yDDKFePWckAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real Toxicity Prompts\n",
        "\n",
        "Visando a operacionalização da avaliação de toxicidade na geração de texto de LLMs, o dataset RealToxicityPrompts (RTP) foi criado. O RTP contém diversos textos para uso como prompt em LLMs. Os prompts são divididos em níveis de toxicidade, visando avaliar o quão propenso o modelo é em gerar textos tóxicos a partir de entradas com diferentes níveis de toxicidade."
      ],
      "metadata": {
        "id": "lG_sVRBViLbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando o dataset do Hugging Face\n",
        "rtp_dataset = load_dataset(\"allenai/real-toxicity-prompts\", split=\"train\").shuffle(seed=42)[:20000]\n",
        "\n",
        "rtp_dataset['prompt'][0]"
      ],
      "metadata": {
        "id": "YIpGL_2h8n59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A avaliação da toxicidade do RTP foi realizada com a API da [Perspective AI](https://perspectiveapi.com/). Nesse notebook não utilizaremos essa API e sim o modelo de classificação [Unbiased RoBERTa](https://huggingface.co/unitary/unbiased-toxic-roberta), treinado em datasets específicos para detecção de textos tóxicos, ofensivos e obscenos. Por isso, vamos separar somente o texto dos prompts do dataset, descartando os valores de toxicidade trazidas pelo dataset, e reavaliar a toxicidade."
      ],
      "metadata": {
        "id": "jRLtx2AEko7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraindo somente o texto dos prompts\n",
        "toxic_prompts = [prompt['text'] for prompt in rtp_dataset['prompt']]"
      ],
      "metadata": {
        "id": "het78intkIK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando o modelo de avaliação de toxicidade\n",
        "toxicity = pipeline(model='unitary/unbiased-toxic-roberta', device=device)\n",
        "\n",
        "toxicity('LLMs can reproduce toxic behaviour when prompted with toxic text', top_k=None)"
      ],
      "metadata": {
        "id": "oshymZ-n6YX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilizando um modelo"
      ],
      "metadata": {
        "id": "RceRvMjpnO77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para geração das continuações dos prompts iremos utilizar o pipeline do Hugging Face com a tarefa 'text-generation', a qual por padrão utiliza o modelo [GPT-2](https://huggingface.co/gpt2)."
      ],
      "metadata": {
        "id": "CVNu5eHioq3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = pipeline('text-generation', device=device)\n",
        "llm('Some texts are', max_length=50, pad_token_id=50256, do_sample=False)"
      ],
      "metadata": {
        "id": "zjkDWrIL-mp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.tokenizer.pad_token_id = 50256 # Definindo manualmente o token especial de 'acolchoamento' para permitir a execução em batches\n",
        "continuations = llm(toxic_prompts, do_sample=False, max_new_tokens=20, pad_token_id=50256, batch_size=256)\n",
        "\n",
        "continuations = [continuation[0]['generated_text'] for continuation in continuations]\n",
        "completions = [continuations[i].replace(toxic_prompts[i], '') for i in range(len(continuations))]"
      ],
      "metadata": {
        "id": "1RDk1Hpi_RU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com as continuações geradas, vamos usar o Unbiased RoBERTa para atribuir um valor de toxicidade entre 0 e 1 a cada um dos prompts e continuações geradas. O valor de toxicidade pode ser visto como qual a probabilidade de uma pessoa considerar o texto como tóxico (rude, desreipeitoso ou insensato)"
      ],
      "metadata": {
        "id": "XLRisF3VqjeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_toxicity = toxicity(toxic_prompts, batch_size=1024, top_k=None)\n",
        "continuations_toxicity = toxicity(completions, batch_size=1024, top_k=None)"
      ],
      "metadata": {
        "id": "IMwNfDuiGCYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_toxicity = [item[0]['score'] for item in prompts_toxicity]\n",
        "continuations_toxicity = [item[0]['score'] for item in continuations_toxicity]"
      ],
      "metadata": {
        "id": "hBTYMYY5UY97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma forma interessante de analisar o resultado dessa avaliação é verificar a propensão do modelo em repercutir o nível de toxicidade vista no prompt de entrada. Para isso, vamos dividir os exemplos em 10 conjuntos com base na toxicidade do prompt de entrada. Cada conjunto é uma lista dos valores de toxicidade dos textos gerados para prompts que tenham a toxicidade dentro de uma certa faixa de valores. Essas faixas são: $0 <= x < 0.1$; $0.1 <= x < 0.2$; ...; $0.9<= x < 1$."
      ],
      "metadata": {
        "id": "9knonllSsbaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "buckets = [[] for i in range(10)]\n",
        "\n",
        "for i, toxicity_value in enumerate(prompts_toxicity):\n",
        "    continuation_toxicity = continuations_toxicity[i]\n",
        "    idx = int(10 * toxicity_value)\n",
        "    buckets[idx].append(continuation_toxicity)"
      ],
      "metadata": {
        "id": "tCPt9O17YXpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora para cada faixa vamos definir a média do valor de toxicidade dentre as continuações geradas. Com isso, podemos traçar um gráfico que mostre a relação entre o nível de toxicidade esperado na continuação feita pelo modelo, dado o nível de toxicidade do texto usado como prompt."
      ],
      "metadata": {
        "id": "17wc-ley0Opd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expected_toxicity = [sum(bucket) / len(bucket) for bucket in buckets]"
      ],
      "metadata": {
        "id": "ubZMcTkZZckj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "x_vec = [0.05+i*0.1 for i in range(10)]\n",
        "\n",
        "ax.plot(x_vec, expected_toxicity, label='GPT2', marker='o', linewidth=2)\n",
        "\n",
        "ax.set(ylim=[0, 0.6], xticks=[0.1*i for i in range(10)])\n",
        "ax.grid()\n",
        "ax.set_ylabel(\"Probabilidade de Toxicidade na Continuação\", fontsize=15)\n",
        "ax.set_xlabel(\"Probabilidade de Toxicidade no prompt\", fontsize=15)\n",
        "ax.legend(loc=2, fontsize=20)\n",
        "fig.set_size_inches(12,6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B62kJLrJaT8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Vieses"
      ],
      "metadata": {
        "id": "QCjOgnAEAbEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outro característica indesejada nos textos de LLMs e altamente relacionada aos padrões de linguagem presentes nos dados de treinamento são os vieses. Existem diferentes maneiras pelos quais os vieses podem se manifestar nos textos. De manira geral, os LLMs apresentam vieses na geração desproporcional de textos negativos, injustos ou estereotipados contra um grupo de pessoas ou ideia [[BOLD](https://arxiv.org/pdf/2101.11718.pdf)]. É importante observar que essa métrica não se precupa com a proporção geral de toxicidade gerada pelo modelo, mas sim que a probabilidade de geração de textos desrepeitosos e positivos sejam iguais para todos os grupos. Dessa forma, mesmo um modelo que gere textos ofensivos 90% das vezes, mas de forma igual para diferentes grupos (como cristãos, mulçumanos, ateus, etc.) teria uma boa métrica de viés, conforme a definição anterior."
      ],
      "metadata": {
        "id": "1wOcfYNA0639"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos utilizar o dataset [BOLD](https://huggingface.co/datasets/AlexaAI/bold), o qual consiste em diversos prompts separados por grupos para analise de viés em modelos de linguagem."
      ],
      "metadata": {
        "id": "kiOVx2CJ6lP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bold_dataset = load_dataset(\"AlexaAI/bold\", split=\"train\")"
      ],
      "metadata": {
        "id": "YkIwW7j8AeVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesse exemplo vamos comparar os textos gerados para prompts sobre atores e atrizes."
      ],
      "metadata": {
        "id": "h-aFTFAD-Kbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrando os exemplos sobre os grupos 'atores' e 'atrizes'\n",
        "male_prompts = bold_dataset.filter(lambda x: x['category'] == 'American_actors').shuffle(seed=42)[:500]\n",
        "female_prompts = bold_dataset.filter(lambda x: x['category'] == 'American_actresses').shuffle(seed=42)[:500]\n",
        "\n",
        "male_prompts = [p[0] for p in male_prompts['prompts']]\n",
        "female_prompts = [p[0] for p in female_prompts['prompts']]"
      ],
      "metadata": {
        "id": "7uVcFieiCbtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(male_prompts), len(female_prompts)"
      ],
      "metadata": {
        "id": "Tp504BzFDH1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Novamente utilizaremos o pipeline de 'text-generation' da Hugging Face com o modelo padrão GPT 2 para gerar as continuações dos prompts."
      ],
      "metadata": {
        "id": "ry03_2DG-nW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = pipeline('text-generation', device=device)\n",
        "llm.tokenizer.pad_token_id = 50256\n",
        "\n",
        "generated_female = llm(female_prompts, do_sample=False, max_new_tokens=20, batch_size=100)\n",
        "generated_female = [gen[0]['generated_text'] for gen in generated_female]\n",
        "generated_female = [generated_female[i].replace(female_prompts[i], '') for i in range(len(generated_female))]\n",
        "\n",
        "generated_male = llm(male_prompts, do_sample=False, max_new_tokens=20, batch_size=100)\n",
        "generated_male = [gen[0]['generated_text'] for gen in generated_male]\n",
        "generated_male = [generated_male[i].replace(male_prompts[i], '') for i in range(len(generated_male))]"
      ],
      "metadata": {
        "id": "vTVHerjYdFrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos avaliar a diferença na polaridade dos textos gerados para cada grupo. A polaridade mensura o sentimento do texto em negativo, neutro ou positivo. A partir da diferença entre a proporção de textos em cada categoria de polaridade, podemos observar a presença ou não de viés entre os grupos analisados."
      ],
      "metadata": {
        "id": "SSN0Ar6j-u07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regard = evaluate.load('regard', module_type='measurement')\n",
        "\n",
        "female_eval = regard.compute(data=generated_female)\n",
        "male_eval = regard.compute(data=generated_male)"
      ],
      "metadata": {
        "id": "9-PNp0zRDNT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir dos valores de polaridade dos textos gerados, calculamos a porcentagem de textos que foram classificados como negativo, neutro ou positivo para cada grupo"
      ],
      "metadata": {
        "id": "XCVRS5C5H72n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "female_count = {'positive': 0, 'neutral': 0, 'negative': 0}\n",
        "for eval in female_eval['regard']:\n",
        "    best_score = {'score': 0}\n",
        "    for score in eval:\n",
        "        if score['score'] > best_score['score'] and score['label'] != 'other':\n",
        "            best_score = score\n",
        "    female_count[best_score['label']] += 1 / len(female_eval['regard'])\n",
        "\n",
        "male_count = {'positive': 0, 'neutral': 0, 'negative': 0}\n",
        "for eval in male_eval['regard']:\n",
        "    best_score = {'score': 0}\n",
        "    for score in eval:\n",
        "        if score['score'] > best_score['score'] and score['label'] != 'other':\n",
        "            best_score = score\n",
        "    male_count[best_score['label']] += 1 / len(male_eval['regard'])"
      ],
      "metadata": {
        "id": "6-edajANCj3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "female_count, male_count"
      ],
      "metadata": {
        "id": "_dN2VJgyGHtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "legend_handles = []\n",
        "for i, eval in enumerate([male_count, female_count]):\n",
        "    neg = eval['negative']\n",
        "    neut = eval['neutral']\n",
        "    pos = eval['positive']\n",
        "\n",
        "    handle = ax.bar(i, neg, color='tab:red')\n",
        "    ax.bar(i, neut, color='lightsteelblue', bottom=neg)\n",
        "    ax.bar(i, pos, color='tab:green', bottom=neg + neut)\n",
        "\n",
        "    legend_handles.append(handle)\n",
        "\n",
        "ax.set_ylabel(\"Proporção de Textos Gerados\", fontsize=15)\n",
        "ax.legend(labels=['Negative', 'Neutral', 'Positive'], fontsize=15, bbox_to_anchor=(1.6, 1), borderaxespad=0)\n",
        "plt.xticks(ticks=[0, 1], labels=['Male', 'Female'], fontsize=12)\n",
        "plt.yticks(ticks=[0.1*i for i in range(11)])\n",
        "fig.set_size_inches(4,6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jUaIQmLGYzy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O gráfico acima apresenta a comparação entre as proporções de textos negativos, neutros e positivos gerados a partir de prompts sobre atores e atrizes. Podemos observar que a proporção de textos negativos é ligeiramente maior para atrizes do que atores, apresentando um pequeno viés por parte do modelo."
      ],
      "metadata": {
        "id": "mSJHL2etAQqO"
      }
    }
  ]
}